{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93156cae",
   "metadata": {},
   "source": [
    "#### Date: 21-06-2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b705d1d",
   "metadata": {},
   "source": [
    "#### Challenge: multiclass classification for <code>ChestMNIST</code> dataset (https://arxiv.org/abs/2110.14795)\n",
    "#### Tutorial: https://github.com/MedMNIST/MedMNIST/blob/main/examples/getting_started.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "5e37f240",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import medmnist as medm\n",
    "from medmnist import INFO, Evaluator\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "e4908d5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_flag = 'chestmnist'\n",
    "download = True\n",
    "\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 256\n",
    "lr = 0.001\n",
    "\n",
    "info = INFO[data_flag]\n",
    "task = info['task']\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "\n",
    "DataClass = getattr(medm, info['python_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5ece0916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "medmnist.dataset.ChestMNIST"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "eef1d63a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "0e106dfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'python_class': 'ChestMNIST',\n",
       " 'description': 'The ChestMNIST is based on the NIH-ChestXray14 dataset, a dataset comprising 112,120 frontal-view X-Ray images of 30,805 unique patients with the text-mined 14 disease labels, which could be formulized as a multi-label binary-class classification task. We use the official data split, and resize the source images of 1×1024×1024 into 1×28×28.',\n",
       " 'url': 'https://zenodo.org/record/6496656/files/chestmnist.npz?download=1',\n",
       " 'MD5': '02c8a6516a18b556561a56cbdd36c4a8',\n",
       " 'task': 'multi-label, binary-class',\n",
       " 'label': {'0': 'atelectasis',\n",
       "  '1': 'cardiomegaly',\n",
       "  '2': 'effusion',\n",
       "  '3': 'infiltration',\n",
       "  '4': 'mass',\n",
       "  '5': 'nodule',\n",
       "  '6': 'pneumonia',\n",
       "  '7': 'pneumothorax',\n",
       "  '8': 'consolidation',\n",
       "  '9': 'edema',\n",
       "  '10': 'emphysema',\n",
       "  '11': 'fibrosis',\n",
       "  '12': 'pleural',\n",
       "  '13': 'hernia'},\n",
       " 'n_channels': 1,\n",
       " 'n_samples': {'train': 78468, 'val': 11219, 'test': 22433},\n",
       " 'license': 'CC BY 4.0'}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "42c69e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'multi-label, binary-class'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task # we should choose 'multi-label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "7235caf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a preprocessing func to normalize data\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[.5], std=[.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "9fb975d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /home/jiawei/.medmnist/chestmnist.npz\n",
      "Using downloaded and verified file: /home/jiawei/.medmnist/chestmnist.npz\n",
      "Using downloaded and verified file: /home/jiawei/.medmnist/chestmnist.npz\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "train_dataset = DataClass(split='train', transform=data_transform, download=download)\n",
    "test_dataset = DataClass(split='test', transform=data_transform, download=download)\n",
    "\n",
    "pil_dataset = DataClass(split='train', download=download)\n",
    "\n",
    "# encapsulate data into dataloader form\n",
    "train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "435d3c11",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiawei/anaconda3/envs/r_env/lib/python3.10/site-packages/medmnist/utils.py:25: FutureWarning: `multichannel` is a deprecated argument name for `montage`. It will be removed in version 1.0. Please use `channel_axis` instead.\n",
      "  montage_arr = skimage_montage(sel_img, multichannel=(n_channels == 3))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=140x140>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualization as per montage\n",
    "train_dataset.montage(length=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6d29fe",
   "metadata": {},
   "source": [
    "#### Then, we define a baseline model for illustration using ResNet18 (28) NN arch, object function and optimizer that we use to classify.\n",
    "#### Constructing ResNet18 from Scratch in Python https://www.kaggle.com/code/ivankunyankin/resnet18-from-scratch-using-pytorch/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "979cc243",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, identity_downsample=None, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "        x += identity\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class ResNet18(nn.Module):\n",
    "    \n",
    "    def __init__(self, image_channels, num_classes):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        #resnet layers\n",
    "        self.layer1 = self.__make_layer(64, 64, stride=1)\n",
    "        self.layer2 = self.__make_layer(64, 128, stride=2)\n",
    "        self.layer3 = self.__make_layer(128, 256, stride=2)\n",
    "        self.layer4 = self.__make_layer(256, 512, stride=2)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def __make_layer(self, in_channels, out_channels, stride):\n",
    "        \n",
    "        identity_downsample = None\n",
    "        if stride != 1:\n",
    "            identity_downsample = self.identity_downsample(in_channels, out_channels)\n",
    "            \n",
    "        return nn.Sequential(\n",
    "            Block(in_channels, out_channels, identity_downsample=identity_downsample, stride=stride), \n",
    "            Block(out_channels, out_channels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x \n",
    "    \n",
    "    def identity_downsample(self, in_channels, out_channels):\n",
    "        \n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1), \n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "resn18_model = ResNet18(n_channels, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4771333",
   "metadata": {},
   "source": [
    "#### Perform traning and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2d772cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 307/307 [36:55<00:00,  7.22s/it]\n",
      "100%|█████████████████████████████████████████| 307/307 [36:45<00:00,  7.18s/it]\n",
      "100%|█████████████████████████████████████████| 307/307 [37:40<00:00,  7.36s/it]\n"
     ]
    }
   ],
   "source": [
    "# define the loss function and optimizer\n",
    "# define loss function and optimizer\n",
    "if task == \"multi-label, binary-class\":\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Try to use\n",
    "#optimizer_adam = optim.Adam(resn18_model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "optimizer = optim.SGD(resn18_model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "# train ChestMNIST Data for 3 epochs\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    \n",
    "    resn18_model.train()\n",
    "    for inputs, targets in tqdm(train_loader):\n",
    "        # forward + backward + optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = resn18_model(inputs)\n",
    "        \n",
    "        if task == 'multi-label, binary-class':\n",
    "            targets = targets.to(torch.float32)\n",
    "            loss = criterion(outputs, targets)\n",
    "        else:\n",
    "            targets = targets.squeeze().long()\n",
    "            loss = criterion(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f06be16",
   "metadata": {},
   "source": [
    "#### Save the model and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e15da2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p manuel_git_repo/notebooks/resnet18_chestmnist_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d3fbe973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save states of the trained model\n",
    "torch.save(resn18_model.state_dict(), \"manuel_git_repo/notebooks/resnet18_chestmnist_test/3epochs_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cd0020d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154534 manuel_git_repo/resnet18_chestmnist_test/3epochs_model\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "wc -l manuel_git_repo/resnet18_chestmnist_test/3epochs_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "dc2bcf19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22433, 14]) torch.Size([22433, 14])\n"
     ]
    }
   ],
   "source": [
    "# predict the test dataset using the train model (3 epochs)\n",
    "resn18_model.eval()\n",
    "y_true = torch.tensor([])\n",
    "y_score = torch.tensor([])\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        outputs = resn18_model(inputs)\n",
    "        if task == 'multi-label, binary-class':\n",
    "            targets = targets.to(torch.float32)\n",
    "            # use softmax instead of standard normalization\n",
    "            outputs = outputs.softmax(dim=-1)\n",
    "        # \"else\" part could be ignored since we don't evaluate other types of datasets \n",
    "        else:\n",
    "            targets = targets.squeeze().long()\n",
    "            outputs = outputs.softmax(dim=-1)\n",
    "            targets = targets.float().resize_(len(targets), 1)\n",
    "        y_true = torch.cat((y_true, targets), 0)\n",
    "        y_score = torch.cat((y_score, outputs), 0)\n",
    "print(y_true.size(), y_score.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a19e49",
   "metadata": {},
   "source": [
    "#### How was AUC and ACC calculated in the source code of <code>MedMNIST</code>?: https://github.com/MedMNIST/MedMNIST/blob/d8422ac64028488133fd21ff54372729e12bbaba/medmnist/evaluator.py\n",
    "#### Answer: There were only the arithmetic means of AUC/ACC at all labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "15a8937a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22433, 14) (22433, 14)\n",
      "==> Evaluating ...\n",
      "Metrics(AUC=0.5947056293188621, ACC=0.9463481732906242)\n"
     ]
    }
   ],
   "source": [
    "#y_true = y_true.numpy()\n",
    "#y_score = y_score.detach().numpy()    \n",
    "print(np.shape(y_true), np.shape(y_score))\n",
    "evaluator = Evaluator(data_flag, 'test')\n",
    "metrics = evaluator.evaluate(y_score)    \n",
    "print('==> Evaluating ...')\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7f2071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train ChestMNIST Data for 1 epoch\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    \n",
    "    resn18_model.train()\n",
    "    for inputs, targets in tqdm(train_loader):\n",
    "        # forward + backward + optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = resn18_model(inputs)\n",
    "        \n",
    "        if task == 'multi-label, binary-class':\n",
    "            targets = targets.to(torch.float32)\n",
    "            loss = criterion(outputs, targets)\n",
    "        else:\n",
    "            targets = targets.squeeze().long()\n",
    "            loss = criterion(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8f8cffa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the one-epoch model to an external file\n",
    "torch.save(resn18_model.state_dict(), \"manuel_git_repo/notebooks/resnet18_chestmnist_test/1epoch_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "676e6e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22433, 14]) torch.Size([22433, 14])\n",
      "torch.Size([22433, 14]) torch.Size([22433, 14])\n",
      "==> Evaluating ...\n",
      "Metrics(AUC=0.6075437745499812, ACC=0.9457049881870458)\n"
     ]
    }
   ],
   "source": [
    "# evaluate the 1 epoch model using ACC and AUC as metrics\n",
    "# predict the test dataset using the train model (1 epoch)\n",
    "resn18_model.eval()\n",
    "y_true = torch.tensor([])\n",
    "y_score = torch.tensor([])\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        outputs = resn18_model(inputs)\n",
    "        if task == 'multi-label, binary-class':\n",
    "            targets = targets.to(torch.float32)\n",
    "            # use softmax instead of standard normalization\n",
    "            outputs = outputs.softmax(dim=-1)\n",
    "        # \"else\" part could be ignored since we don't evaluate other types of datasets \n",
    "        else:\n",
    "            targets = targets.squeeze().long()\n",
    "            outputs = outputs.softmax(dim=-1)\n",
    "            targets = targets.float().resize_(len(targets), 1)\n",
    "        y_true = torch.cat((y_true, targets), 0)\n",
    "        y_score = torch.cat((y_score, outputs), 0)\n",
    "print(y_true.size(), y_score.size())    \n",
    "print(np.shape(y_true), np.shape(y_score))\n",
    "evaluator = Evaluator(data_flag, 'test')\n",
    "metrics = evaluator.evaluate(y_score)    \n",
    "print('==> Evaluating ...')\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "42e06d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208ff283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the 1 epoch model using ACC and AUC as metrics, with softmax normalization\n",
    "# predict the test dataset using the train model (1 epoch)\n",
    "resn18_model.eval()\n",
    "y_true = torch.tensor([])\n",
    "y_score = torch.tensor([])\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        outputs = resn18_model(inputs)\n",
    "        if task == 'multi-label, binary-class':\n",
    "            targets = targets.to(torch.float32)\n",
    "            # use softmax instead of standard normalization\n",
    "            outputs = outputs.softmax(dim=-1)\n",
    "        # \"else\" part could be ignored since we don't evaluate other types of datasets \n",
    "        else:\n",
    "            targets = targets.squeeze().long()\n",
    "            outputs = outputs.(dim=-1)\n",
    "            targets = targets.float().resize_(len(targets), 1)\n",
    "        y_true = torch.cat((y_true, targets), 0)\n",
    "        y_score = torch.cat((y_score, outputs), 0)\n",
    "print(y_true.size(), y_score.size())    \n",
    "print(np.shape(y_true), np.shape(y_score))\n",
    "evaluator = Evaluator(data_flag, 'test')\n",
    "metrics = evaluator.evaluate(y_score)    \n",
    "print('==> Evaluating ...')\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6e146fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22433, 14]) torch.Size([22433, 14])\n",
      "torch.Size([22433, 14]) torch.Size([22433, 14])\n",
      "==> Evaluating ...\n",
      "Metrics(AUC=0.6676118807276293, ACC=0.9471378262890767)\n"
     ]
    }
   ],
   "source": [
    "# evaluate the 1 epoch model using ACC and AUC as metrics, with sigmoid normalization\n",
    "# predict the test dataset using the train model (3 epochs)\n",
    "resn18_model.eval()\n",
    "y_true = torch.tensor([])\n",
    "y_score = torch.tensor([])\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        outputs = resn18_model(inputs)\n",
    "        if task == 'multi-label, binary-class':\n",
    "            targets = targets.to(torch.float32)\n",
    "            # use softmax instead of standard normalization\n",
    "            outputs = sigmoid(outputs)\n",
    "        # \"else\" part could be ignored since we don't evaluate other types of datasets \n",
    "        else:\n",
    "            targets = targets.squeeze().long()\n",
    "            outputs = sigmoid(outputs)\n",
    "            targets = targets.float().resize_(len(targets), 1)\n",
    "        y_true = torch.cat((y_true, targets), 0)\n",
    "        y_score = torch.cat((y_score, outputs), 0)\n",
    "print(y_true.size(), y_score.size())    \n",
    "print(np.shape(y_true), np.shape(y_score))\n",
    "evaluator = Evaluator(data_flag, 'test')\n",
    "metrics = evaluator.evaluate(y_score)    \n",
    "print('==> Evaluating ...')\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "13bd4e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22433, 14]) torch.Size([22433, 14])\n",
      "torch.Size([22433, 14]) torch.Size([22433, 14])\n",
      "==> Evaluating ...\n",
      "Metrics(AUC=0.6676118807276293, ACC=0.9474466825021811)\n"
     ]
    }
   ],
   "source": [
    "# evaluate the 1 epoch model using ACC and AUC as metrics, without normalization\n",
    "# predict the test dataset using the train model (1 epoch)\n",
    "resn18_model.eval()\n",
    "y_true = torch.tensor([])\n",
    "y_score = torch.tensor([])\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        outputs = resn18_model(inputs)\n",
    "        if task == 'multi-label, binary-class':\n",
    "            targets = targets.to(torch.float32)\n",
    "            # use softmax instead of standard normalization\n",
    "            #outputs = sigmoid(outputs)\n",
    "        # \"else\" part could be ignored since we don't evaluate other types of datasets \n",
    "        else:\n",
    "            targets = targets.squeeze().long()\n",
    "            #outputs = sigmoid(outputs)\n",
    "            targets = targets.float().resize_(len(targets), 1)\n",
    "        y_true = torch.cat((y_true, targets), 0)\n",
    "        y_score = torch.cat((y_score, outputs), 0)\n",
    "print(y_true.size(), y_score.size())    \n",
    "print(np.shape(y_true), np.shape(y_score))\n",
    "evaluator = Evaluator(data_flag, 'test')\n",
    "metrics = evaluator.evaluate(y_score)    \n",
    "print('==> Evaluating ...')\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3bb621",
   "metadata": {},
   "source": [
    "#### Conclusion: as regards to AUC values, no normalization = sigmoid normalization > softmax normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6382d6c1",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63322a76",
   "metadata": {},
   "source": [
    "#### Write a function to evaluate the precision/recall of the 1-epoch/3-epochs model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c9ebd7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, \\\n",
    "precision_recall_fscore_support\n",
    "from typing import Any, List, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "5cd39670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_score(model: nn.Module, test_loader: torch.utils.data.DataLoader, threshold=0.5): \n",
    "    model.eval()\n",
    "    y_true = torch.tensor([])\n",
    "    y_score = torch.tensor([])\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            logits = model(inputs)\n",
    "            targets = targets.to(torch.float32)\n",
    "            # use softmax instead of standard normalization\n",
    "            #outputs = outputs.softmax(dim=-1)\n",
    "            predictions = (outputs > threshold).int()\n",
    "            predictions = (logits > threshold).int()\n",
    "            # Concatenate this batch to the complete results\n",
    "            y_true = torch.cat((y_true, targets), 0)\n",
    "            y_score = torch.cat((y_score, predictions), 0)\n",
    "        print(y_true.shape, y_score.shape)\n",
    "        precision, recall, f1, support = precision_recall_fscore_support(y_true, y_score)\n",
    "        return precision, recall, f1\n",
    "        \"\"\"\n",
    "        # averaged auc value per label, unnecessary here\n",
    "        auc = 0\n",
    "        for i in range(y_score.shape[1]):\n",
    "            try:\n",
    "                label_auc = roc_auc_score(y_true[:, i], y_score[:, i])\n",
    "                precision, recall, f1, support = precision_recall_fscore_support(y_true[:, i])\n",
    "                auc += label_auc\n",
    "            except ValueError:\n",
    "                # If all labels are of the same value, an ValueError is thrown\n",
    "                # Equal to: auc += 0.0\n",
    "                pass\n",
    "        return auc / y_score.shape[1]\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a654f423",
   "metadata": {},
   "source": [
    "#### Start the evaluation for precision and recall value for the 3-epoch ResNet18 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f60d8c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the three-epoch model\n",
    "resn18_model_3 = ResNet18(n_channels, n_classes)\n",
    "resn18_model_3.load_state_dict(torch.load(\"manuel_git_repo/notebooks/resnet18_chestmnist_test/3epochs_model\"))\n",
    "# load the one-epoch model\n",
    "resn18_model_1 = ResNet18(n_channels, n_classes)\n",
    "resn18_model_1.load_state_dict(torch.load(\"manuel_git_repo/notebooks/resnet18_chestmnist_test/1epoch_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "b60b5d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22433, 14]) torch.Size([22433, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiawei/anaconda3/envs/r_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22433, 14]) torch.Size([22433, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiawei/anaconda3/envs/r_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "eval_3epochs = get_test_score(model=resn18_model_3, test_loader=test_loader)\n",
    "eval_1epoch =  get_test_score(model=resn18_model_1, test_loader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "a35443e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision for a model trained for 3 epochs epochs: precision-[0.2        0.         0.51666667 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ] recall-[0.00041322 0.         0.02251271 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ] f1-[0.00082474 0.         0.04314544 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "precision for a model trained for 1 epoch epochs: precision-[0.2        0.         0.51666667 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ] recall-[0.00041322 0.         0.02251271 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ] f1-[0.00082474 0.         0.04314544 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "for res, epc in zip((eval_3epochs, eval_3epochs), ('3 epochs', '1 epoch')):\n",
    "    print(f\"precision for a model trained for {epc} epochs: precision-{res[0]} recall-{res[1]} f1-{res[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae96a4f",
   "metadata": {},
   "source": [
    "#### Conclusion: the results appear to be extremely low in all the three metrics. Therefore, there was either problem in the training data or the input processing. When the threshold is 0.5, it behaves like that. \n",
    "#### Moreover, the 1-epoch model appears to be different. Let's try the 3-epochs model only at the moment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33590a7",
   "metadata": {},
   "source": [
    "#### Instead, I whould like to try a pre-trained ResNet18 model for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "3610d97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "47c6532a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18_trained = models.resnet18(pretrained=True)\n",
    "resnet18_trained.fc # check the dimension of the original pretrained fully-connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "4d22b402",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18_trained.fc = nn.Linear(512, 14, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea277758",
   "metadata": {},
   "source": [
    "#### Since the final fully-connected layer was modified, we would need to train on the final layer again (1 epoch for this time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "0c46e2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight torch.Size([64, 3, 7, 7])\n",
      "bn1.weight torch.Size([64])\n",
      "bn1.bias torch.Size([64])\n",
      "layer1.0.conv1.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.0.bn1.weight torch.Size([64])\n",
      "layer1.0.bn1.bias torch.Size([64])\n",
      "layer1.0.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.0.bn2.weight torch.Size([64])\n",
      "layer1.0.bn2.bias torch.Size([64])\n",
      "layer1.1.conv1.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.1.bn1.weight torch.Size([64])\n",
      "layer1.1.bn1.bias torch.Size([64])\n",
      "layer1.1.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.1.bn2.weight torch.Size([64])\n",
      "layer1.1.bn2.bias torch.Size([64])\n",
      "layer2.0.conv1.weight torch.Size([128, 64, 3, 3])\n",
      "layer2.0.bn1.weight torch.Size([128])\n",
      "layer2.0.bn1.bias torch.Size([128])\n",
      "layer2.0.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "layer2.0.bn2.weight torch.Size([128])\n",
      "layer2.0.bn2.bias torch.Size([128])\n",
      "layer2.0.downsample.0.weight torch.Size([128, 64, 1, 1])\n",
      "layer2.0.downsample.1.weight torch.Size([128])\n",
      "layer2.0.downsample.1.bias torch.Size([128])\n",
      "layer2.1.conv1.weight torch.Size([128, 128, 3, 3])\n",
      "layer2.1.bn1.weight torch.Size([128])\n",
      "layer2.1.bn1.bias torch.Size([128])\n",
      "layer2.1.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "layer2.1.bn2.weight torch.Size([128])\n",
      "layer2.1.bn2.bias torch.Size([128])\n",
      "layer3.0.conv1.weight torch.Size([256, 128, 3, 3])\n",
      "layer3.0.bn1.weight torch.Size([256])\n",
      "layer3.0.bn1.bias torch.Size([256])\n",
      "layer3.0.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "layer3.0.bn2.weight torch.Size([256])\n",
      "layer3.0.bn2.bias torch.Size([256])\n",
      "layer3.0.downsample.0.weight torch.Size([256, 128, 1, 1])\n",
      "layer3.0.downsample.1.weight torch.Size([256])\n",
      "layer3.0.downsample.1.bias torch.Size([256])\n",
      "layer3.1.conv1.weight torch.Size([256, 256, 3, 3])\n",
      "layer3.1.bn1.weight torch.Size([256])\n",
      "layer3.1.bn1.bias torch.Size([256])\n",
      "layer3.1.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "layer3.1.bn2.weight torch.Size([256])\n",
      "layer3.1.bn2.bias torch.Size([256])\n",
      "layer4.0.conv1.weight torch.Size([512, 256, 3, 3])\n",
      "layer4.0.bn1.weight torch.Size([512])\n",
      "layer4.0.bn1.bias torch.Size([512])\n",
      "layer4.0.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "layer4.0.bn2.weight torch.Size([512])\n",
      "layer4.0.bn2.bias torch.Size([512])\n",
      "layer4.0.downsample.0.weight torch.Size([512, 256, 1, 1])\n",
      "layer4.0.downsample.1.weight torch.Size([512])\n",
      "layer4.0.downsample.1.bias torch.Size([512])\n",
      "layer4.1.conv1.weight torch.Size([512, 512, 3, 3])\n",
      "layer4.1.bn1.weight torch.Size([512])\n",
      "layer4.1.bn1.bias torch.Size([512])\n",
      "layer4.1.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "layer4.1.bn2.weight torch.Size([512])\n",
      "layer4.1.bn2.bias torch.Size([512])\n",
      "fc.weight torch.Size([14, 512])\n",
      "fc.bias torch.Size([14])\n"
     ]
    }
   ],
   "source": [
    "for name, param in resnet18_trained.named_parameters():\n",
    "    print(name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "5eb7c503",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18_trained.layer1.requires_grad=False\n",
    "resnet18_trained.layer2.requires_grad=False\n",
    "resnet18_trained.layer3.requires_grad=False\n",
    "resnet18_trained.layer4.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "9c9caff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 307/307 [31:01<00:00,  6.06s/it]\n"
     ]
    }
   ],
   "source": [
    "# train only the final fully connected layer using ChestMNIST Data for 1 epoch\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    \n",
    "    resnet18_trained.train()\n",
    "    for inputs, targets in tqdm(train_loader):\n",
    "        # forward + backward + optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = resnet18_trained(inputs.repeat(1, 3, 1, 1))\n",
    "        if task == 'multi-label, binary-class':\n",
    "            targets = targets.to(torch.float32)\n",
    "            loss = criterion(outputs, targets)\n",
    "        else:\n",
    "            targets = targets.squeeze().long()\n",
    "            loss = criterion(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "4a0d63a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_score(model: nn.Module, test_loader: torch.utils.data.DataLoader, threshold=0.5): \n",
    "    model.eval()\n",
    "    y_true = torch.tensor([])\n",
    "    y_score = torch.tensor([])\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            # convert a one-channel input into a three-channel one\n",
    "            inputs = inputs.repeat(1, 3, 1, 1)\n",
    "            outputs = model(inputs)\n",
    "            #print(outputs.shape)\n",
    "            logits = model(inputs)\n",
    "            targets = targets.to(torch.float32)\n",
    "            # use softmax instead of standard normalization\n",
    "            #outputs = outputs.softmax(dim=-1)\n",
    "            predictions = (outputs > threshold).int()\n",
    "            predictions = (logits > threshold).int()\n",
    "            # Concatenate this batch to the complete results\n",
    "            y_true = torch.cat((y_true, targets), 0)\n",
    "            y_score = torch.cat((y_score, predictions), 0)\n",
    "        print(y_true.shape, y_score.shape)\n",
    "        precision, recall, f1, support = precision_recall_fscore_support(y_true, y_score)\n",
    "        return precision, recall, f1\n",
    "        \"\"\"\n",
    "        # averaged auc value per label, unnecessary here\n",
    "        auc = 0\n",
    "        for i in range(y_score.shape[1]):\n",
    "            try:\n",
    "                label_auc = roc_auc_score(y_true[:, i], y_score[:, i])\n",
    "                precision, recall, f1, support = precision_recall_fscore_support(y_true[:, i])\n",
    "                auc += label_auc\n",
    "            except ValueError:\n",
    "                # If all labels are of the same value, an ValueError is thrown\n",
    "                # Equal to: auc += 0.0\n",
    "                pass\n",
    "        return auc / y_score.shape[1]\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "3ed18457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22433, 14]) torch.Size([22433, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiawei/anaconda3/envs/r_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "eval_pretrained =  get_test_score(model=resnet18_trained, test_loader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "8b8350db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision for a pretrained ResNet18 model at ImageNet: precision-[0.11393229 0.02574257 0.12909344 0.1842714  0.04283802 0.06145251\n",
      " 0.01114206 0.0706619  0.04842083 0.01591512 0.0297542  0.01794505\n",
      " 0.03227176 0.00224115] recall-[0.1446281  0.06701031 0.37073348 0.06069071 0.0282436  0.1071161\n",
      " 0.04958678 0.21763085 0.48380355 0.08716707 0.04518664 0.3121547\n",
      " 0.10354223 0.23809524] f1-[0.12745812 0.03719599 0.19150333 0.0913085  0.03404255 0.0780994\n",
      " 0.0181956  0.10668467 0.08803118 0.02691589 0.03588144 0.03393903\n",
      " 0.04920686 0.0044405 ]\n"
     ]
    }
   ],
   "source": [
    "res = eval_pretrained\n",
    "print(f\"precision for a pretrained ResNet18 model at ImageNet: precision-{res[0]} recall-{res[1]} f1-{res[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "5f4403fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg recall: 0.1654, avg precision: 0.0659\n"
     ]
    }
   ],
   "source": [
    "print(f\"avg recall: {round(np.mean(res[1]), 4)}, avg precision: {round(np.mean(res[2]), 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7996545",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
